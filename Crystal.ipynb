{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Crystal.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMupPR4lSCQz7/u9er/x6vs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohamedAliRashad/Crystal/blob/master/Crystal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEJEMgVgikV-",
        "colab_type": "text"
      },
      "source": [
        "# **Welcome to Crystal**\n",
        "\n",
        "[GitHub](https://github.com/MohamedAliRashad/Crystal) - [YouTube](https://www.youtube.com/watch?v=96E6hB2DE5w)\n",
        "\n",
        "We wish here to achieve the following\n",
        "\n",
        "\n",
        "1.   Provide a working (fully tested) environment for trying out **Crystal**\n",
        "2.   Explain the different stages of our pipeline for anyone interested in adding to it or trying new things.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2htM3s9xISOy",
        "colab_type": "text"
      },
      "source": [
        "## Try it out yourself\n",
        "\n",
        "For those who are just interested in using **Crystal**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REWUDM4hFJgJ",
        "colab_type": "text"
      },
      "source": [
        "### Set Up Everything Needed\n",
        "\n",
        "**Note:** The working directory will be Crystal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93gFGZSREn61",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/MohamedAliRashad/Crystal.git\n",
        "%cd Crystal\n",
        "!pip3 install -r requirements.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2RH83CMQGm-",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Downlad the video from YouTube\n",
        "video_url = \"https://www.youtube.com/watch?v=W2X_p4DDK3s\" #@param {type:\"string\"}\n",
        "\n",
        "!youtube-dl -f 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/mp4' {video_url} -o video"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF3koCLxT8-i",
        "colab_type": "text"
      },
      "source": [
        "### Just, Run ^ ^\n",
        "\n",
        "**Note:** Output will be in ```content``` with the name of ```video.mp4```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-y9x0rBUSaR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from main import main\n",
        "\n",
        "# main only require 2 arguments:\n",
        "#       Video Path\n",
        "#       Output Path\n",
        "# And that's it\n",
        "\n",
        "main(\"video.mp4\", \"/content/\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbOvq1-7UFPx",
        "colab_type": "text"
      },
      "source": [
        "## Learn how it ticks\n",
        "\n",
        "For those who want to learn or improve on the existing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FD2jazRgHt1G",
        "colab_type": "text"
      },
      "source": [
        "### First Step: Extract frames\n",
        "\n",
        "**Note:** Audio will be extracted too."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1vtqBmhaCZD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "import os.path as osp\n",
        "import imageio\n",
        "from tqdm import tqdm\n",
        "\n",
        "video_file = \"video.mp4\"\n",
        "rate = None\n",
        "\n",
        "out_dir = osp.splitext(osp.basename(video_file))[0]\n",
        "os.mkdir(out_dir)\n",
        "\n",
        "reader = imageio.get_reader(video_file)\n",
        "meta_data = reader.get_meta_data()\n",
        "fps = meta_data['fps']\n",
        "n_frames = meta_data['nframes']\n",
        "\n",
        "j = 0\n",
        "for i, img in tqdm(enumerate(reader), total=n_frames):\n",
        "    if rate is None or i % int(round(fps / rate)) == 0:\n",
        "        imageio.imsave(osp.join(out_dir, '%08d.jpg' % j), img)\n",
        "        j = j + 1\n",
        "\n",
        "# !ffmpeg -i video.mp4 -vn -acodec copy output-audio.aac\n",
        "\n",
        "# !rm {video_file}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kp7h6KjmV7pu",
        "colab_type": "text"
      },
      "source": [
        "### Second Step: Run DAIN\n",
        "\n",
        "**DAIN** or ```Depth Aware Frame Interpolation``` is the first module in our pipeline where we smooth out the video by increasing its frame rate.\n",
        "\n",
        "It basically consists of a Depth Estimator generated with Deep Learning (HourGlass and PWCNets) aimed in providing a better optical flow of the motion happening in the video so we can interpolate intermediate frames (in a smarter way).\n",
        "\n",
        "[Project](https://sites.google.com/view/wenbobao/dain) **|** [Paper](http://arxiv.org/abs/1904.00830)\n",
        "\n",
        "\n",
        "(**default:** doubling the FPS)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAiIb03Lqe2g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "from pathlib import Path\n",
        "from imageio import imread, imsave\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Get the DAIN Module ready\n",
        "def load_DAIN():\n",
        "    # Let the magic happen\n",
        "    from DAIN.DAIN import DAIN\n",
        "    module = DAIN()\n",
        "    \n",
        "    # load the weights online\n",
        "    from torch.hub import load_state_dict_from_url\n",
        "    state_dict = load_state_dict_from_url(\"http://vllab1.ucmerced.edu/~wenbobao/DAIN/best.pth\")\n",
        "    module.load_state_dict(state_dict)\n",
        "\n",
        "    return module\n",
        "\n",
        "# Forward the video frames through the model\n",
        "# Note: inframes and outframes need to be a Path object\n",
        "def infer_DAIN(model, meta_data, inframes, outframes):\n",
        "\n",
        "    model.cuda() # use the GPU\n",
        "\n",
        "    frames = sorted(inframes.glob(\"*.jpg\"))\n",
        "\n",
        "    # Scale the frames down while mainting the aspect ratio so no stackoverflow happens in GPU\n",
        "    # scale_precent is how much do you want from the original size where 100 is the size not changed\n",
        "    scale_precent = 100\n",
        "    width = int(meta_data[\"size\"][0] * scale_precent / 100)\n",
        "    height = int(meta_data[\"size\"][1] * scale_precent / 100)\n",
        "    dim = (width, height)\n",
        "    model.eval()\n",
        "\n",
        "    j = 0\n",
        "    for i in tqdm(range(len(frames) - 1)):\n",
        "\n",
        "        image1 = cv2.resize(imread(frames[i]), dim, interpolation=cv2.INTER_AREA)\n",
        "        image2 = cv2.resize(imread(frames[i + 1]), dim, interpolation=cv2.INTER_AREA)\n",
        "        \n",
        "        image1 = imread(frames[i])\n",
        "        image2 = imread(frames[i + 1])\n",
        "        \n",
        "        X0 = torch.from_numpy(np.transpose(image1, (2, 0, 1)).astype(\"float32\") / 255.0).type(torch.cuda.FloatTensor)\n",
        "        X1 = torch.from_numpy(np.transpose(image2, (2, 0, 1)).astype(\"float32\") / 255.0).type(torch.cuda.FloatTensor)\n",
        "        y_ = torch.FloatTensor()\n",
        "\n",
        "        intWidth = X0.size(2)\n",
        "        intHeight = X0.size(1)\n",
        "        channel = X0.size(0)\n",
        "\n",
        "        if intWidth != ((intWidth >> 7) << 7):\n",
        "            intWidth_pad = ((intWidth >> 7) + 1) << 7  # more than necessary\n",
        "            intPaddingLeft = int((intWidth_pad - intWidth) / 2)\n",
        "            intPaddingRight = intWidth_pad - intWidth - intPaddingLeft\n",
        "        else:\n",
        "            intWidth_pad = intWidth\n",
        "            intPaddingLeft = 32\n",
        "            intPaddingRight = 32\n",
        "\n",
        "        if intHeight != ((intHeight >> 7) << 7):\n",
        "            intHeight_pad = ((intHeight >> 7) + 1) << 7  # more than necessary\n",
        "            intPaddingTop = int((intHeight_pad - intHeight) / 2)\n",
        "            intPaddingBottom = intHeight_pad - intHeight - intPaddingTop\n",
        "        else:\n",
        "            intHeight_pad = intHeight\n",
        "            intPaddingTop = 32\n",
        "            intPaddingBottom = 32\n",
        "\n",
        "        pader = torch.nn.ReplicationPad2d(\n",
        "            [intPaddingLeft, intPaddingRight, intPaddingTop, intPaddingBottom]\n",
        "        )\n",
        "\n",
        "        torch.set_grad_enabled(False)\n",
        "        X0 = torch.unsqueeze(X0, 0)\n",
        "        X1 = torch.unsqueeze(X1, 0)\n",
        "\n",
        "        X0 = pader(X0).cuda()\n",
        "        X1 = pader(X1).cuda()\n",
        "\n",
        "        y_s, offset, filter = model(torch.stack((X0, X1), dim=0))\n",
        "\n",
        "        y_ = y_s[1]\n",
        "\n",
        "        X0 = X0.data.cpu().numpy()\n",
        "        y_ = y_.data.cpu().numpy()\n",
        "        offset = [offset_i.data.cpu().numpy() for offset_i in offset]\n",
        "        filter = [filter_i.data.cpu().numpy() for filter_i in filter] if filter[0] is not None else None\n",
        "        X1 = X1.data.cpu().numpy()\n",
        "\n",
        "        X0 = np.transpose(\n",
        "            255.0\n",
        "            * X0.clip(0, 1.0)[\n",
        "                0,\n",
        "                :,\n",
        "                intPaddingTop : intPaddingTop + intHeight,\n",
        "                intPaddingLeft : intPaddingLeft + intWidth,\n",
        "            ],\n",
        "            (1, 2, 0),\n",
        "        )\n",
        "        y_ = np.transpose(\n",
        "            255.0\n",
        "            * y_.clip(0, 1.0)[\n",
        "                0,\n",
        "                :,\n",
        "                intPaddingTop : intPaddingTop + intHeight,\n",
        "                intPaddingLeft : intPaddingLeft + intWidth,\n",
        "            ],\n",
        "            (1, 2, 0),\n",
        "        )\n",
        "        offset = [\n",
        "            np.transpose(\n",
        "                offset_i[\n",
        "                    0,\n",
        "                    :,\n",
        "                    intPaddingTop : intPaddingTop + intHeight,\n",
        "                    intPaddingLeft : intPaddingLeft + intWidth,\n",
        "                ],\n",
        "                (1, 2, 0),\n",
        "            )\n",
        "            for offset_i in offset\n",
        "        ]\n",
        "        filter = (\n",
        "            [\n",
        "                np.transpose(\n",
        "                    filter_i[\n",
        "                        0,\n",
        "                        :,\n",
        "                        intPaddingTop : intPaddingTop + intHeight,\n",
        "                        intPaddingLeft : intPaddingLeft + intWidth,\n",
        "                    ],\n",
        "                    (1, 2, 0),\n",
        "                )\n",
        "                for filter_i in filter\n",
        "            ]\n",
        "            if filter is not None\n",
        "            else None\n",
        "        )\n",
        "        X1 = np.transpose(\n",
        "            255.0\n",
        "            * X1.clip(0, 1.0)[\n",
        "                0,\n",
        "                :,\n",
        "                intPaddingTop : intPaddingTop + intHeight,\n",
        "                intPaddingLeft : intPaddingLeft + intWidth,\n",
        "            ],\n",
        "            (1, 2, 0),\n",
        "        )\n",
        "\n",
        "        imsave(os.path.join(str(outframes), str(j).zfill(6) + \".jpg\"), cv2.resize(image1, meta_data[\"size\"], interpolation=cv2.INTER_AREA))\n",
        "        imsave(os.path.join(str(outframes), str(j+1).zfill(6) + \".jpg\"), cv2.resize(np.round(y_).astype(np.uint8), meta_data[\"size\"], interpolation=cv2.INTER_AREA))\n",
        "        j = j + 2\n",
        "        \n",
        "    imsave(os.path.join(str(outframes), str(j).zfill(6) + \".jpg\"), cv2.resize(image2, meta_data[\"size\"], interpolation=cv2.INTER_AREA))\n",
        "    meta_data[\"fps\"] = meta_data[\"fps\"]*2\n",
        "\n",
        "    return meta_data\n",
        "\n",
        "\n",
        "model = load_DAIN()\n",
        "inframes = Path(\"./video/\")\n",
        "outframes = Path(\"./tmp/\")\n",
        "outframes.mkdir(parents=True, exist_ok=True)\n",
        "meta_data = infer_DAIN(model, meta_data, inframes, outframes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTbflBHjXjwn",
        "colab_type": "text"
      },
      "source": [
        "### Third Step: Run EDVR\n",
        "\n",
        "\n",
        "**EDVR** or ```Enhanced Deformables for Video Restoration``` is the second module in our pipeline where we enhance the video by increasing its resolution with sharp and deblurring filters.\n",
        "\n",
        "**EDVR** is the winner in all four tracks of NTIRE19 video restoration and enhancement challenges with his **Pyramid, Cascading and Deformable (PCD) alignment module** and **Temporal and Spatial Attention (TSA) fusion module**.\n",
        "\n",
        "[Project](https://xinntao.github.io/projects/EDVR) **|** [Paper](https://arxiv.org/abs/1905.02716)\n",
        "\n",
        "(**default:** Two Stage Enhancement)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3gRtrUaW-LW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "\n",
        "import cv2\n",
        "import torch\n",
        "from torch.hub import load_state_dict_from_url\n",
        "from tqdm import tqdm\n",
        "import os.path as osp\n",
        "from pathlib import Path\n",
        "\n",
        "from SR_EDVR.EDVR_arch import EDVR\n",
        "from SR_EDVR.utils.data_utils import index_generation, read_img_seq\n",
        "from SR_EDVR.utils.util import flipx4_forward, mkdirs, single_forward, tensor2img, preProcess\n",
        "\n",
        "Weights = {\n",
        "    \"EDVR_REDS_SR_L\": \"https://drive.google.com/uc?export=download&id=1PYULZmtpsmY4Wx8M9f4owdLIwcwQFEmi\",\n",
        "    \"EDVR_REDS_deblur_L\": \"https://drive.google.com/uc?export=download&id=1ZCl0aU8isEnUCsUYv9rIZZQrGo7vBFUH\",\n",
        "    \"EDVR_REDS_deblurcomp_L\": \"https://drive.google.com/uc?export=download&id=1SGVehpZt4WL_X8Jh6blyqmHpc8DdImgv\",\n",
        "    \"EDVR_REDS_SRblur_L\": \"https://drive.google.com/uc?export=download&id=18ev7Zx_10-C8-0tAVAe_BpYeLHpr_ChE\",\n",
        "    \"EDVR_Vimeo90K_SR_L\": \"https://drive.google.com/uc?export=download&id=1I7x87ee3E1DoFVgMxX09nfIb2tdUdE3x\",\n",
        "    \"EDVR_REDS_SR_Stage2\": \"https://drive.google.com/uc?export=download&id=1kfArevFT8hzbUT2QWXFmUl983LTebQGP\",\n",
        "    \"EDVR_REDS_deblur_Stage2\": \"https://drive.google.com/uc?export=download&id=1Y1y6v40dL74Kgf5fxbGd0QC010LFCBYz\",\n",
        "    \"EDVR_REDS_deblurcomp_Stage2\": \"https://drive.google.com/uc?export=download&id=1G466gQ1rRl8MUKSEbtaR0U5xgIWdsG66\",\n",
        "    \"EDVR_REDS_SRblur_Stage2\": \"https://drive.google.com/uc?export=download&id=13c-VxMdf8h7MGX-_y4xamxo1hhOMYzsH\",\n",
        "}\n",
        "\n",
        "\n",
        "def SuperResolution(inframes, outframes, stage, data_mode, use_gpu=True):\n",
        "    \"\"\"\n",
        "\tPerform a Super Resolution step on a frames folder\n",
        "\n",
        "\tArgs\n",
        "\t----\n",
        "\t\tinframes(str, Path): folder with the frames to enhance\n",
        "\t\toutframes(str, Path): the output directory\n",
        "\t\tstage(int): determine the stage used (1 or 2)\n",
        "\t\tdata_mode(str): the process wanted\n",
        "\t\t\tVid4: SR\n",
        "\t\t\tREDS4: sharp_bicubic (SR-clean), blur_bicubic (SR-blur);\n",
        "\t\t\t\t\tblur (deblur-clean), blur_comp (deblur-compression).\n",
        "\t\"\"\"\n",
        "\n",
        "\tflip_test = False\n",
        "\tinframes = Path(inframes)\n",
        "\toutframes = str(outframes)\n",
        "\tdevice = \"cuda\" if torch.cuda.is_available() and use_gpu else \"cpu\"\n",
        "\n",
        "\t################## Model ##################\n",
        "\tmodel_name = None\n",
        "\tif data_mode == \"Vid4\":\n",
        "\t\t\tif stage == 1:\n",
        "\t\t\t\t\tmodel_name = \"EDVR_Vimeo90K_SR_L\"\n",
        "\t\t\telse:\n",
        "\t\t\t\t\traise ValueError(\"Vid4 does not support stage 2.\")\n",
        "\telif data_mode == \"sharp_bicubic\":\n",
        "\t\t\tif stage == 1:\n",
        "\t\t\t\t\tmodel_name = \"EDVR_REDS_SR_L\"\n",
        "\t\t\telse:\n",
        "\t\t\t\t\tmodel_name = \"EDVR_REDS_SR_Stage2\"\n",
        "\telif data_mode == \"blur_bicubic\":\n",
        "\t\t\tif stage == 1:\n",
        "\t\t\t\t\tmodel_name = \"EDVR_REDS_SRblur_L\"\n",
        "\t\t\telse:\n",
        "\t\t\t\t\tmodel_name = \"EDVR_REDS_SRblur_Stage2\"\n",
        "\telif data_mode == \"blur\":\n",
        "\t\t\tif stage == 1:\n",
        "\t\t\t\t\tmodel_name = \"EDVR_REDS_deblur_L\"\n",
        "\t\t\telse:\n",
        "\t\t\t\t\tmodel_name = \"EDVR_REDS_deblur_Stage2\"\n",
        "\telif data_mode == \"blur_comp\":\n",
        "\t\t\tif stage == 1:\n",
        "\t\t\t\t\tmodel_name = \"EDVR_REDS_deblurcomp_L\"\n",
        "\t\t\telse:\n",
        "\t\t\t\t\tmodel_name = \"EDVR_REDS_deblurcomp_Stage2\"\n",
        "\telse:\n",
        "\t\t\traise NotImplementedError\n",
        "\n",
        "\tprint(\"Model Used: \", model_name)\n",
        "\n",
        "\tif data_mode == \"Vid4\":\n",
        "\t\t\tN_in = 7  # use N_in images to restore one HR image\n",
        "\telse:\n",
        "\t\t\tN_in = 5\n",
        "\n",
        "\tpredeblur, HR_in = False, False\n",
        "\tback_RBs = 40\n",
        "\tif data_mode == \"blur_bicubic\":\n",
        "\t\t\tpredeblur = True\n",
        "\tif data_mode == \"blur\" or data_mode == \"blur_comp\":\n",
        "\t\t\tpredeblur, HR_in = True, True\n",
        "\tif stage == 2:\n",
        "\t\t\tHR_in = True\n",
        "\t\t\tback_RBs = 20\n",
        "\n",
        "\t# Initialize the model\n",
        "\tmodel = EDVR(128, N_in, 8, 5, back_RBs, predeblur=predeblur, HR_in=HR_in)\n",
        "\n",
        "\t#### evaluation\n",
        "\tcrop_border = 0\n",
        "\tborder_frame = N_in // 2  # border frames when evaluate\n",
        "\t# temporal padding mode\n",
        "\tif data_mode in (\"Vid4\", \"sharp_bicubic\"):\n",
        "\t\t\tpadding = \"new_info\"\n",
        "\telse:\n",
        "\t\t\tpadding = \"replicate\"\n",
        "\tsave_imgs = True\n",
        "\n",
        "\t#### set up the models\n",
        "\tstate_dict = load_state_dict_from_url(Weights[model_name], model_dir=model_name)\n",
        "\tmodel.load_state_dict(state_dict, strict=True)\n",
        "\tmodel.eval()\n",
        "\tmodel = model.to(device)\n",
        "\n",
        "\timg_path_l = sorted(inframes.glob(\"*\"))\n",
        "\n",
        "\t# preprocess images (needed for blurred models)\n",
        "\tif predeblur:\n",
        "\t\t\tpreProcess(img_path_l, 16)\n",
        "\telse:\n",
        "\t\t\tpreProcess(img_path_l, 4)\n",
        "\n",
        "\timgs_LQ = read_img_seq(inframes)\n",
        "\tmax_idx = len(img_path_l)\n",
        "\n",
        "\t# process each image\n",
        "\tfor img_idx, img_path in enumerate(tqdm(img_path_l)):\n",
        "\t\t\timg_name = osp.splitext(osp.basename(img_path))[0]\n",
        "\t\t\tselect_idx = index_generation(img_idx, max_idx, N_in, padding=padding)\n",
        "\t\t\timgs_in = (\n",
        "\t\t\t\t\timgs_LQ.index_select(0, torch.LongTensor(select_idx))\n",
        "\t\t\t\t\t.unsqueeze(0)\n",
        "\t\t\t\t\t.to(device)\n",
        "\t\t\t)\n",
        "\n",
        "\t\t\tif flip_test:\n",
        "\t\t\t\t\toutput = flipx4_forward(model, imgs_in)\n",
        "\t\t\telse:\n",
        "\t\t\t\t\toutput = single_forward(model, imgs_in)\n",
        "\t\t\toutput = tensor2img(output.squeeze(0))\n",
        "\n",
        "\t\t\tif save_imgs:\n",
        "\t\t\t\t\tcv2.imwrite(osp.join(outframes, \"{}.jpg\".format(img_name)), output)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\ttemp = Path(\"./tmp/\")\n",
        "\ttemp2 = Path(\"./tmp2/\")\n",
        "\ttemp2.mkdir(parents=True, exist_ok=True)\n",
        "\tSuperResolution(tmp, temp2, 1, \"sharp_bicubic\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcZunjyDU6Nc",
        "colab_type": "text"
      },
      "source": [
        "### Fourth Step: Rebuild the Video\n",
        "\n",
        "**Note** The video will be Audioless (we are still working on it)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkkhcY24Ia5E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_video(frames_dir, save_dir, meta_data, audio_path=None):\n",
        "    \"\"\"\n",
        "    Construct video from frames\n",
        "    \n",
        "    Args\n",
        "    ----\n",
        "        frames_dir(str, Path): folder path to frames\n",
        "        save_dir(str, Path):\n",
        "\n",
        "    \"\"\"\n",
        "    save_dir = Path(save_dir)\n",
        "    out_path = str(save_dir / meta_data[\"name\"])\n",
        "\n",
        "    # Use ffmpeg to reconstruct the video\n",
        "    ffmpeg.input(\n",
        "        str(frames_dir), format=\"image2\", vcodec=\"mjpeg\", framerate=meta_data[\"fps\"]\n",
        "    ).output(out_path, crf=17, vcodec=\"libx264\").run(capture_stdout=True)\n",
        "\n",
        "build_video(tmp2, \"/content/\", meta_data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}